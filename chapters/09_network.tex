\chapter{Network Security}
\section{Information Gathering}
The first step in attacking a networked environment is always to gather
information on how the network is structured, and which connections, hosts and
segments exist. It is important to know which services exist, as they might
represent possible attack vectors. The next step is identifying vulnerabilities,
and exploiting them. This often includes searching for systems with known
vulnerabilities. What follows is often some measure to ensure permanent control,
such as the installation of a rootkit or backdoor. This process may then be
repeated to gain control to additional systems.

\subsection{DNS}
DNS exposes a lot of information about a network. Brute-forcing reverse DNS
lookups for a targets IP range reveals domains or hostnames, which often reveal
the purpose or service of a system.

\subsection{whois}
\texttt{whois} provides information about an IP range, specifically details
about the organization and administrator responsible. Privacy regulations make
it possible to vastly reduce the information visible nowadays.

\subsection{traceroute}
Network topology can be examined using \texttt{traceroute}. This tool uses ICMP
packets with increasing TTL to find the address of each intermediate router
between the source and a target address. Executing this from multiple hosts to
multiple targets enables mapping the complete network.

\subsection{SMTP}
\subsection{Sniffing}
Tools such as tcpdump or wireshark allow passive recording and decoding of
network traffic. This allows direct retrieval of information transmitted in
plaintext, and a lot of information even with encrypted communication through
metadata.

\subsection{Scanning}
Active scanning of hosts and networks is possible through tools like
\texttt{nmap}. The goal is usually to find out on which TCP or UDP ports there
are applications listening.

\paragraph{TCP connect scanning} simply tries to establish a TCP connection with
the target. The typical packet sequence in a successful case would be
\texttt{SYN}, \texttt{SYN/ACK}, \texttt{ACK}. If no application is listening,
the sequence would be \texttt{SYN}, \texttt{RST/ACK}. This scanning technique is
fast and easy to execute, even for non-superuser users. Disadvantages include
that this only works for TCP, and is easy to detect even on application level.

\paragraph{TCP \texttt{SYN} scanning} is an alternative where no complete TCP
session is ever created. If the port is open and the target responds with
\texttt{SYN/ACK}, the connection is immediately aborted with \texttt{RST}, not
completing the three-way-handshake. This never creates a connection that the
application receives, making it a lot more difficult to detect. This however
also only works for TCP, and additionally requires superuser privileges to
execute.

\section{Attacks}
The TCP/IP family of protocols was not designed with security as an objective.
Various different attacks are possible:

\subsection{Routing Attacks}
Attacks on routing protocols can facilitate redirection of traffic through the
attackers systems, or blackhole the traffic for a denial of service attack.
Routing protocols currently in place are not very security focused, OSPF for
example uses simple authentication using plaintext passwords and MD5 hashes,
BGP4 relies on manual filters for incoming information.

Inside a subnet, techniques such as ARP spoofing are possible, which allow
redirection of traffic to the attackers machine just by responding to ARP
queries for the corresponding IP address (or the gateway). DHCP replies can be
forged in a similar way.

\subsection{DNS Attacks}
DNS attacks are of special importance since basically all internet communication
relies on DNS during initialization of the connection. If the attacker controls
the DNS server, they can redirect traffic intended for a specific domain to a
malicious server. One method of attacking is \emph{DNS cache poisoning}:

DNS makes use of a hierarchical architecture, and various levels of caching to
ease load on the root and authoritative servers. The attacker targets such a
caching server by triggering it to refresh the cache, for example by querying a
targeted domain, and then immediately delivering a malicious response which then
gets stored in the cache. While refreshing a cache entry, the server sends a
request with a specific \texttt{query ID} to the upstream server. For a response
to be accepted, the \texttt{query ID} has to match, but it is not verified in
any other way if the reply indeed originates from an upstream server instead of
a malicious attacker.

The \texttt{query ID} is traditionally simply incremented each request, which
means the attacker could easily guess it by querying the server for a domain, to
which they own the authoritative name server, and assuming that the
\texttt{query ID} for the to-be-poisoned request will be a subsequent number.
The attacker will send multiple replies with different \texttt{query ID}s, with
a high probability that one of them matches. If the reply arrives at the cache
before the legitimate reply, the attack has succeeded.

The challenges to the attacker are so far:
\begin{itemize}
    \item The entry must not be in the cache already
    \item The correct \texttt{query ID} must be guessed
    \item The response must arrive before the legitimate response
\end{itemize}

The most straightforward fix is to implement cryptographically secure random
\texttt{query ID}s.

It is however still possible to take over an entire nameserver/zone: The goal
here is to corrupt the cache entry for the authoritative nameserver in the
caching nameserver. Requests to the victim nameserver can be made for any
non-existing subdomain the target nameserver is responsible for. In the forged
response, a \texttt{NS} record is present containing the domain of the
legitimate authoritative nameserver, but the accompanying \texttt{A} record
points to a server the attacker controls. The ability to use arbitrary
subdomains that will not be cached makes guessing the \texttt{query ID}
feasible, combined with the fact that it is only 16bit long.

Additional mitigations include using the port number of the request as an
additional identifier, but this does only increase the effort in guessing, it
does not solve the underlying problem. A long term solution is DNSSEC
(\cref{sec:dnssec}).


\subsection{Man in the Middle Attacks}
The objective of a man-in-the-middle attack is to infiltrate an existing or new
connection. Both endpoints think they communicate with each other, when they are
actually both communicating with the attacker. The attacker can then both record
and modify the traffic, and this can even work with encrypted connections. If
the attacker spoofs the IP of a target web server for example, the client
establishes a TLS connection, and the real web server receives an incoming TLS
connection, but both connections are separate and are terminated at the
attacker. A mitigation is exchanging certificates while the connection is
established. The certificate is signed (transitively) by a root CA, which has a
public key that is known to the client in advance, usually by distribution with
the operating system or browser. The user can then be warned if the server did
not send a certificate, or sent a certificate that is not signed by a known CA.

\subsection{Denial of Service Attacks}
A wide variety of DOS attacks exist at every level of a system. All those have
one goal in common: to deplete some resource of the system, making it
unavailable.

In network settings, DDOS, \textit{distributed} denial of service, is often
considered. Here, the attacker makes use of a large number of hosts, via a
botnet for example, to overwhelm the target with traffic from many sources. This
makes this type of attack possible even if every attacking system is a lot less
powerful than the target. Those attacks usually target the network connection or
server resources (CPU, memory, etc) of the victim.

Multiple types of network DOS attacks exist, but one example is \emph{SYN
    flooding}: The client sends a massive number of TCP \texttt{SYN} packets to
    the target, but never answers with \texttt{ACK}, establishing a connection.
    This results in the server saving a status for each half-established
    connection, which starves its resources.

Protection against SYN flooding is provided by \emph{SYN cookies}: It avoids
saving state when a \texttt{SYN} packet arrives. This is done by using the
sequence number in the TCP packet, which gets incremented whenever a packet is
sent. One does not however has to start at 1 or increment by 1. The server
inserts a value into the sequence number of the \texttt{SYN/ACK} which can be
used later to recognize that packet. This is generated using a hash function
that takes information like the current time, the host IP and port as well as a
server secret as input. The client is expected to increment this by 1 when
returning the ACK. When the server receives it, it can ``decrypt'' or
reconstruct the value to verify that the client has sent a \texttt{SYN} before.

General protection measures against DOS attacks include special hardware
appliances which monitor the traffic and protect against unusual amounts of
traffic, overprovisioning of servers and use of scalable cloud resources and
CDNs.

Difficult to protect against are \emph{low-and-slow} attacks, which try to slow
down a server by sending specially crafted packets which are known to trigger
slow or performance intensive operations on the target.

\section{Security Mechanisms}

In addition to the aforementioned protective measures against specific attacks,
this section goes into detail on DNSSEC, and the more general topics of
firewalls and intrusion detection.

\subsection{DNSSEC}
\label{sec:dnssec}
The basic idea of DNSSEC is that the authoritative servers sign the records
using asymmetric cryptography. The caching resolvers can then verify the
signatures. The required key-hierarchy lends itself to the hierarchical model
which is already in place with DNS. A server signs its records with a \emph{zone
signing key}, which is signed using the servers \emph{key signing key}. A hash
of the KSK is available at the parent zones nameserver, making it possible to
verify the identity of the server up to the root zone.

Solutions such as DoT and DoH introduce TLS and HTTPS as transports for DNS,
with the goal of privacy. It is however questionable how useful such measures
are, as they only delegate the problem to the operator of the DNS server.

\subsection{Firewalls}
Firewalls reduce the attack surface of a network by reducing its exposure to the
internet as a whole. A firewall is a network component which separates multiple
network segments from each other. Traffic between the networks is controlled
according to a security policy.

Firewalls often filter packets, which is done on multiple levels:
\begin{itemize}
    \item Virus-/Java-/ActiveX filter for email and HTTP: This provides a
          similar approach as local antivirus software may take, but at a
          centralized location. Those filters can however be easily
          circumvented, using encryption or even just unusual compression
          methods.
    \item Content filters, which scan for specific addresses or keywords
    \item Anti-Spoofing filters which remove packets with forged addresses (such
          as IP spoofing)
\end{itemize}

A firewall can however perform a lot of additional functions:
\begin{itemize}
    \item Logging
    \item Authentication
    \item Enforcement of data protection
    \item Network Address Translation
\end{itemize}

All those functions and requirements indicate that the firewall is closely
related and integrated with routers, gateways on multiple levels, logging
facilities and services providing authentication and security policies.

\subsubsection{Security policies}
An important decision in firewall configuration is the \emph{base policy}. It
specifies how traffic is handled, for which no more specific rule exists.

\emph{Default-Deny} specifies that access is usually prohibited unless it is
explicitly allowed. The list of allowed services is provided by the
administrator. This makes deployment of new services difficult, and may
frustrate users since a lot of services are not available.

The alternative is \emph{Default-Allow}, where access is usually permitted
unless explicitly prohibited. This has the disadvantage from a security
perspective that all potentially dangerous services have to be blocked, which is
usually not known in advance.

\subsubsection{Filtering}
Network level filtering can be done on layer 3 or layer 4: Layer 3 filtering
works on IP datagrams and is based on the source and destination addresses.
Layer 4 filtering can additionally take the port number and direction of
connection (in case of TCP) into account.

Application level filtering is sometimes referred to as \emph{deep packet
inspection}, and can make protocol specific decisions such as allowing only
certain HTTP methods or URLs, or block emails to a specific domain even when
being transmitted to a common mail server.

Packet filters are usually fully transparent and as such independent from the
user, and can be implemented highly performant. Disadvantages include poor
logging capabilities, difficult to set-up rules, and difficult to filter
protocols which may require stateful inspection, and usually no interpretation
of streamed data.

\subsubsection{Application Gateways / Proxies}
Gateways or proxy servers prohibit direct transfer of data via lower levels, but
require a proxy for each application protocol, which implements that protocol
and inspects the data, before forwarding it. This automatically realizes the
default-deny policy. It also makes it possible to authenticate the user, handle
user sessions and interpret data streams. The application specific nature
facilitates detailed logging and convenient to set-up filter rules. The gateway
is however not transparent to the user, requires a separate proxy for each
service or application and thus usually operates with lower performance.

\subsection{Intrusion Detection}
